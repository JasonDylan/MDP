{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pulp\n",
    "import inspect\n",
    "\n",
    "\n",
    "\n",
    "def generate_state(I_citys, L_levels, W_workdays, M_servers, x_max_task_num, H_home_of_server, lambd):\n",
    "    \"\"\"\n",
    "    生成一个系统的随机状态，该系统负责将任务分配给员工。\n",
    "    \n",
    "    参数:\n",
    "    I_citys (int): 城市的数量。\n",
    "    L_levels (int): 等级的数量。\n",
    "    W_workdays (int): 员工距离放假的最大工作天数, 一般为7。\n",
    "    M_servers (int): 员工的数量。\n",
    "    x_max_task_num (int): 每个城市的最大任务数(本函数中未直接使用)。\n",
    "    H_home_of_server (list[int]): 代表每个员工家所在城市的列表。\n",
    "    lambd (np.array): 一个二维数组 (I_citys x_max_task_num L_levels)，代表每个城市和等级的任务到达率。\n",
    "\n",
    "    返回:\n",
    "    tuple: 包含任务分布矩阵和员工状态列表的元组。\n",
    "\n",
    "    城市中的任务矩阵，员工状态列表[(员工所在地，距离放假时间)]\n",
    "    \"\"\"\n",
    "\n",
    "    # 生成任务分布矩阵 (n_il)，其维度为 I_citys x_max_task_num L_levels  \n",
    "    n_il = np.zeros((I_citys, L_levels), dtype=int)  # 用零初始化矩阵\n",
    "    for i_city in range(I_citys):  # 遍历城市\n",
    "        for l_level in range(L_levels):  # 遍历等级\n",
    "            # 为每个城市和等级分配一个基于泊松分布的随机数, 表示随机状态生成的任务数量？# TODO\n",
    "            n_il[i_city, l_level] = np.random.poisson(lambd[i_city, l_level])\n",
    "    S0_tasks = n_il  # 任务分布矩阵 (I_citys x_max_task_num L_levels), [0, +∞)\n",
    "    \n",
    "    # 初始化一个列表来保存每个员工的状态，给\n",
    "    S1_servers = []\n",
    "    for m_server in range(M_servers):  # 遍历所有员工\n",
    "\n",
    "        # 为员工 'm_server' 随机选择距离放假的工作日数，取值范围 [0, W_workdays]\n",
    "        w_m = np.random.randint(0, W_workdays + 1)\n",
    "\n",
    "        # 根据距离放假天数w_m，得到位置i_m，如果距离放假0，则在家里，否则随机一个位置。\n",
    "        # i_m [1, I_citys]\n",
    "        if w_m == W_workdays:  # 如果距离放假时间为 0 天，即今天放假，则员工所在城市为家所在城市\n",
    "            i_m = H_home_of_server[m_server]\n",
    "        else:  # 否则，为员工随机选择一 个非家乡城市工作\n",
    "            i_m = np.random.randint(1, I_citys + 1)\n",
    "            \n",
    "        # 将员工的状态作为元组（城市，距离放假的工作日数）添加到列表中\n",
    "        S1_servers.append((i_m, w_m))\n",
    "    \n",
    "    # 将任务分布矩阵和员工状态列表合并成一个状态元组\n",
    "    S = (S0_tasks, S1_servers)    # ((I_citys x_max_task_num L_levels), (M_servers x_max_task_num 1))\n",
    "    return S  # 返回生成的状态\n",
    "\n",
    "\n",
    "def aggreg_state(S_t, Z_cluster_num, X, M_servers, I_citys, L_levels):\n",
    "    # 函数2\n",
    "    # 定义一个函数，用于根据给定的参数将复杂的状态 S 压缩成一个简化的状态 barS。\n",
    "    # 输入:\n",
    "    #   S: 当前状态，一个复杂的结构，包含两部分信息：\n",
    "    #      - 一个数组，表示每个城市每个等级的数量(n_il)。\n",
    "    #      - 一个列表，表示服务员和他们服务的城市及工作日(i_m, w_m)\n",
    "    #   Z_cluster_num: 一个整数，表示将城市分成多少个聚类。\n",
    "    #   X: 一个整数，用于计算 N 矩阵中的元素值。\n",
    "    #   M_servers: 服务员的总数。\n",
    "    #   I_citys: 城市的总数。\n",
    "    #   L_levels: 等级的总数。\n",
    "    # 输出:\n",
    "    #   barS: 一个元组，表示压缩后的状态，包含以下三个部分：\n",
    "    #         - N: 一个二维数组，表示每个聚类的等级之和。\n",
    "    #         - g: 一个数组，表示每个聚类的状态。\n",
    "    #         - w: 一个整数，表示第一个服务员的工作日数。\n",
    "\n",
    "    # 计算正在工作的服务员的数量, S[1][m_server][1]即S1_servers[m_server][1]即w_m\n",
    "    barM = np.sum([1 for m_server in range(M_servers) if S_t[1][m_server][1] != 0]) # 距离放假时间不等于0\n",
    "    # 根据城市数量和设定的簇数，将城市分成Z个簇\n",
    "    cluster = split_list(I_citys, Z_cluster_num)\n",
    "    # 计算实际的簇数，考虑到可能会有余数\n",
    "    # num_cluster = divide_reminder(I_citys, Z_cluster_num) # 这个可以替换为下面的\n",
    "    num_cluster = np.ceil(I_citys / Z_cluster_num).astype(int) # 向上取整\n",
    "    # 初始化表示各簇状态的数组g\n",
    "    g = np.zeros(num_cluster)\n",
    "    \n",
    "    # 压缩状态的第二部分：计算每个簇的状态\n",
    "    for z_cluster in range(num_cluster):\n",
    "        # 统计每个簇中有多少业务员正在工作 \n",
    "        e_z = np.sum([1 for m_server in range(M_servers) if S_t[1][m_server][0] in cluster[z_cluster]])\n",
    "        # 根据工作的业务员数量设置簇的状态\n",
    "        if e_z == 0:\n",
    "            g[z_cluster] = 0  # 无业务员工作\n",
    "        elif e_z <= barM / num_cluster:\n",
    "            g[z_cluster] = 1  # 工作业务员数量低于或等于平均值\n",
    "        else:\n",
    "            g[z_cluster] = 2  # 工作业务员数量高于平均值\n",
    "    \n",
    "    # 获取第一个业务员的工作量\n",
    "    w = S_t[1][0][1]\n",
    "    # 压缩状态的第一部分：计算每个簇中各等级的数量总和\n",
    "    N = np.zeros((num_cluster, L_levels))  # 初始化N矩阵\n",
    "    \n",
    "    i_all = [i for i in cluster[z_cluster]]\n",
    "    # for z_cluster in range(num_cluster):\n",
    "    #     for l in range(L_levels):\n",
    "    #         # 对每个簇的每个等级，计算其数量总和，但不超过X\n",
    "            \n",
    "    #         N[z_cluster][l] = min(X, \n",
    "    #                               np.sum([S_t[0][i-1][l] for i in cluster[z_cluster]]))\n",
    "    for z_cluster in range(num_cluster):\n",
    "        for l in range(L_levels):\n",
    "            # 对每个簇的每个等级，计算其数量总和，但不超过X\n",
    "            task_sum = sum(float(S_t[0][i-1][l]) for i in cluster[z_cluster])\n",
    "            N[z_cluster][l] = min(X, task_sum)\n",
    "    # 将计算出的N矩阵、簇的状态数组g和第一个业务员的工作量w组合成新的压缩状态barS\n",
    "    barS = (N, g, w)        \n",
    "    return barS\n",
    "\n",
    "# 将城市列表平均分成Z个聚类\n",
    "def split_list(I_citys, Z_cluster_num)->list:\n",
    "    # 创建一个从1到I_citys的城市索引列表\n",
    "    arr_city_idx = list(range(1, I_citys + 1))\n",
    "    # 调用函数处理实际的分割\n",
    "    return split_array_given_array(arr_city_idx, Z_cluster_num)\n",
    "\n",
    "\n",
    "\n",
    "def split_array_given_array(arr_city_idx, Z_cluster_num)->list:\n",
    "    \"\"\"\n",
    "    将输入数组分割成长度为 Z_cluster_num 的子数组列表。如果数组不能被 Z_cluster_num 整除，\n",
    "    那么最后一个子数组将包含所有剩余的元素。\n",
    "    \n",
    "    参数:\n",
    "    arr_city_idx (list): 需要被分割的输入数组。\n",
    "    Z_cluster_num (int): 每个子数组的期望长度。\n",
    "    \n",
    "    返回值:\n",
    "    list: 长度为 Z_cluster_num 的子数组列表，除了可能的最后一个子数组，它包含所有剩余的元素。\n",
    "    \"\"\"\n",
    "    result = []  # 结果列表，用来存储所有的子数组\n",
    "    quotient = len(arr_city_idx) // Z_cluster_num  # 计算整除的商，即完整子数组的数量\n",
    "    remainder = len(arr_city_idx) % Z_cluster_num  # 计算余数，即最后一个子数组的元素数量\n",
    "    \n",
    "    # 划分可以整除的数组部分\n",
    "    for i in range(quotient):\n",
    "        sub_array = arr_city_idx[i * Z_cluster_num:(i + 1) * Z_cluster_num]  # 获取从 i*Z_cluster_num 到 (i+1)*Z_cluster_num 的子数组\n",
    "        result.append(sub_array)  # 将子数组添加到结果列表中\n",
    "    \n",
    "    # 如果有余数，则处理剩余部分\n",
    "    if remainder > 0:\n",
    "        sub_array = arr_city_idx[-remainder:]  # 获取数组最后余数个元素形成的子数组\n",
    "        result.append(sub_array)  # 将子数组添加到结果列表中\n",
    "    \n",
    "    return result  # 返回结果列表\n",
    "\n",
    "def divide_reminder(num, divisor):\n",
    "    \"\"\"\n",
    "    将一个整数除以另一个整数，并将结果向上取整。\n",
    "    \n",
    "    参数:\n",
    "    num (int): 被除数。\n",
    "    divisor (int): 除数。\n",
    "    \n",
    "    返回值:\n",
    "    int: 向上取整后的商。\n",
    "    \"\"\"\n",
    "    quotient = num // divisor  # 计算整除的商\n",
    "    remainder = num % divisor  # 计算余数\n",
    "    \n",
    "    # 如果存在余数，则将商向上取整\n",
    "    if remainder > 0:\n",
    "        quotient += 1  # 余数大于0，商加一\n",
    "    \n",
    "    return quotient  # 返回向上取整后的商\n",
    "\n",
    "def func4(S, mathscr_L):\n",
    "    # 假设 len(mathscr_L) = 7\n",
    "    # 假设 S 和 mathscr_L 的数据结构如所描述\n",
    "    # S 是一个二元组 (S0_tasks, S1_servers)，其中 S0_tasks 是任务矩阵\n",
    "    # mathscr_L 是业务员的等级集合，例如 [l_1, l_2, ..., l_M]\n",
    "    #S0 (I_citys x_max_task_num L_levels)\n",
    "    # 首先计算 N_1 和 N_2\n",
    "    L_levels = 5\n",
    "    N_1 = [sum(1 for l_m, (i_m, w_m) in zip(mathscr_L, S[1]) if l_m == j and w_m != 0) for j in range(1, L_levels+1)]\n",
    "    N_2 = [sum(S[0][i][j] for i in range(len(S[0]))) for j in range(L_levels)]\n",
    "    # 初始化分类后的等级列表\n",
    "    mathcal_L = []\n",
    "    current_class = []\n",
    "    total_N_1 = 0\n",
    "    total_N_2 = 0\n",
    "\n",
    "    for j in range(1, L_levels+1):\n",
    "        total_N_1 += N_1[j-1]\n",
    "        total_N_2 += N_2[j-1]\n",
    "        current_class.append(j)\n",
    "\n",
    "        if total_N_1 <= total_N_2:\n",
    "            # 当 N_1 总和小于等于 N_2 总和时，终止当前类的添加\n",
    "            mathcal_L.append(tuple(current_class))\n",
    "            current_class = []\n",
    "            total_N_1 = 0\n",
    "            total_N_2 = 0\n",
    "\n",
    "    if current_class:\n",
    "        # 添加最后一个类\n",
    "        mathcal_L.append(tuple(current_class))\n",
    "\n",
    "    return mathcal_L, N_1, N_2\n",
    "\n",
    "def func3(S, L_server, H_home_of_server, r1_reward, c1_city_cost, c2):\n",
    "    \"\"\"\n",
    "    生成状态 S 到决策 A 的函数，通过解决线性规划问题来最大化收益 R_total_reward(S, A)。\n",
    "\n",
    "    参数:\n",
    "    S (tuple): 当前状态，包含任务矩阵和服务员信息。\n",
    "    L_server (list): 服务员的等级列表。\n",
    "    H_home_of_server (list): 服务员的家位置列表。\n",
    "    r1_reward (list): 每个等级的收益列表。\n",
    "    c1_city_cost (list of list): I×I 的成本矩阵。\n",
    "    c2 (float): 常数成本。\n",
    "\n",
    "    返回:\n",
    "    list: 最优决策 A，包含每个服务员的位置和等级。\n",
    "    \"\"\"\n",
    "    n_il, servers_info = S\n",
    "    \n",
    "    # n_il, servers_info = S\n",
    "    # M_servers = len(servers_info)  # 服务员数量\n",
    "    # I_citys = len(n_il)          # 城市数量\n",
    "    # # L_max = len(r1_reward) - 1    # 最大等级\n",
    "\n",
    "    M_servers = len(servers_info)  # 服务员数量\n",
    "    I_citys = len(n_il)          # 城市数量\n",
    "    L_max = max(L_server) # 最大等级\n",
    "    # print(f\"{M_servers=}40 {I_citys=}26 {L_max=}5 {L_server=}\")\n",
    "\n",
    "    # 创建问题实例\n",
    "    prob = pulp.LpProblem(\"Optimal_Server_Assignment\", pulp.LpMaximize)\n",
    "\n",
    "    # 定义决策变量 y_{mil} 为二元变量\n",
    "    y = pulp.LpVariable.dicts(\"y\", \n",
    "                              ((m, i, l) for m in range(M_servers) for i in range(I_citys) for l in range(L_max+1)), \n",
    "                              cat=pulp.LpBinary)\n",
    "\n",
    "    # 目标函数\n",
    "    # 假设 L_max 和 r1_reward 已经定义\n",
    "\n",
    "    prob += pulp.lpSum(\n",
    "        r1_reward[l1] * y[m1, i1, l1] - c1_city_cost[servers_info[m1][0]-1][i1] * y[m1, i1, l1]\n",
    "                       for m1 in range(M_servers) for i1 in range(I_citys)\n",
    "                       for l1 in range(0, L_max+1))\\\n",
    "                        - c2 * pulp.lpSum(n_il[i][l-1] - pulp.lpSum(y[m, i, l] for m in range(M_servers)) for i in range(I_citys)\n",
    "                       for l in range(1, L_max+1))\n",
    "\n",
    "    # 添加约束\n",
    "    # 每个服务员只能分配到一个地点和等级\n",
    "    for m in range(M_servers):\n",
    "        prob += pulp.lpSum(y[m, i, l] for i in range(I_citys) for l in range(L_max+1)) == 1\n",
    "    \n",
    "    # 服务员不工作时，分配到家乡的等级0\n",
    "    for m, (im, wm) in enumerate(servers_info):\n",
    "        if wm == 0:\n",
    "            prob += y[m, H_home_of_server[m], 0] == 1\n",
    "\n",
    "    # 服务员工作时，必须分配到合适的等级和城市\n",
    "    for m, (im, wm) in enumerate(servers_info):\n",
    "        if wm > 0:\n",
    "            prob += pulp.lpSum(y[m, i, l] for i in range(I_citys) for l in range(L_server[m], L_max+1) if n_il[i][l-1] > 0) == 1\n",
    "\n",
    "    # 资源使用不超过可用数量\n",
    "    for i in range(I_citys):\n",
    "        for l in range(1, L_max+1):\n",
    "            prob += pulp.lpSum(y[m, i, l] for m in range(M_servers)) <= n_il[i][l-1]\n",
    "\n",
    "    # 求解问题\n",
    "    prob.solve()\n",
    "\n",
    "    # 解析结果\n",
    "    result = [(m, i+1, l) for m in range(M_servers) for i in range(I_citys) for l in range(L_max+1) if pulp.value(y[m, i, l]) == 1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def func5(S, mathcal_L, mathscr_L, N_1, N_2, H_home_of_server, r1_reward, c1_city_cost, c2):\n",
    "    \"\"\"\n",
    "    生成状态 S 到决策 Y_best_allocation 的函数,通过解决线性规划问题来最大化收益 R_total_reward(S, Y_best_allocation)。\n",
    "\n",
    "    参数:\n",
    "    S (tuple): 当前状态,包含任务矩阵和服务员信息。\n",
    "    mathcal_L (list): 分类后的等级列表。\n",
    "    mathscr_L (list): 所有服务员的等级列表\n",
    "    N_1 (list): 每个等级的服务员数量。\n",
    "    N_2 (list): 每个等级的任务数量。\n",
    "    H_home_of_server (list): 服务员的家位置列表。\n",
    "    r1_reward (list): 每个等级的收益列表。\n",
    "    c1_city_cost (list of list): I×I 的成本矩阵。\n",
    "    c2 (float): 常数成本。\n",
    "\n",
    "    返回:\n",
    "    list: 最优决策 Y_best_allocation,包含每个服务员的位置和等级。\n",
    "    R_total_reward, 总收益\n",
    "    \"\"\"\n",
    "    n_il, servers_info = S\n",
    "    M_servers = len(servers_info)  # 服务员数量\n",
    "    I_citys = len(n_il)          # 城市数量\n",
    "    L_max = [max(l) for l in mathcal_L]  # 最大等级\n",
    "    # 步骤1:安排放假的员工回家\n",
    "    C_h = sum(c1_city_cost[servers_info[m][0]-1][H_home_of_server[m]] for m in range(M_servers) if servers_info[m][1] == 0)\n",
    "    \n",
    "    R_total_reward = -C_h  # 初始化总收益为负的回家成本\n",
    "\n",
    "    Y_best_allocation = [None] * M_servers  # 初始化最优决策 Y_best_allocation\n",
    "    \n",
    "    Y_set = []\n",
    "    # 步骤2:对每个等级类独立进行员工分配\n",
    "    for L_set, l_max_L in zip(mathcal_L, L_max):\n",
    "        M_servers_L = [m for m in range(M_servers) if servers_info[m][1] > 0 and mathscr_L[m] in L_set]  # 该等级类下工作的员工集合\n",
    "        I_citys_L = [i for i in range(I_citys) if any(n_il[i][l-1] > 0 for l in L_set)]  # 该等级类下有任务需求的城市集合\n",
    "        \n",
    "        # 创建问题实例\n",
    "        prob = pulp.LpProblem(f\"Optimal_Server_Assignment_Level_{L_set}\", pulp.LpMaximize)\n",
    "\n",
    "        # 定义决策变量 y_{mil} 为二元变量\n",
    "        y = pulp.LpVariable.dicts(\"y\", \n",
    "                                  ((m, i, l) for m in M_servers_L for i in I_citys_L for l in L_set), \n",
    "                                  cat=pulp.LpBinary)\n",
    "\n",
    "        # 定义一个辅助变量,存储服务员分配的收益和城市成本\n",
    "        # 这个表达式计算了每个服务员分配到每个城市每个等级任务的收益和成本\n",
    "\n",
    "        reward_cost_expr = pulp.lpSum(\n",
    "            r1_reward[l1-1] * y[m1, i1, l1] - c1_city_cost[servers_info[m1][0]-1][i1] * y[m1, i1, l1]\n",
    "            for m1 in M_servers_L for i1 in I_citys_L for l1 in L_set\n",
    "        )\n",
    "\n",
    "        # 目标函数\n",
    "        # 根据等级类型选择\n",
    "        # 如果每个等级的服务员数量小于等于任务数量,则最大化收益减去未分配任务的惩罚\n",
    "        if sum(N_1[l-1] for l in L_set) <= sum(N_2[l-1] for l in L_set):\n",
    "            prob += reward_cost_expr - c2 * pulp.lpSum(\n",
    "                n_il[i][l-1] - pulp.lpSum(y[m, i, l] for m in M_servers_L)\n",
    "                for i in I_citys_L for l in L_set\n",
    "            )\n",
    "        # 否则,只最大化收益\n",
    "        else:\n",
    "            prob += reward_cost_expr\n",
    "\n",
    "        # 添加约束\n",
    "        for m in M_servers_L: \n",
    "            # 每个工作中的服务员 m,要求其被分配到城市 i 提供的服务等级 l 必须不低于他自身的服务等级 L_server[m]\n",
    "            # 且只能被分配到一个城市提供一种等级的服务。\n",
    "            prob += pulp.lpSum(y[m, i, l] for i in I_citys_L for l in L_set if l >= mathscr_L[m]) == 1\n",
    "\n",
    "        # 遍历每个城市和等级\n",
    "        for i in I_citys_L:\n",
    "            for l in L_set:\n",
    "                # 如果每个等级的服务员数量小于等于任务数量,则添加约束:\n",
    "                # 每个城市每个等级分配的服务员数量小于等于该等级任务数量\n",
    "                if sum(N_1[l-1] for l in L_set) <= sum(N_2[l-1] for l in L_set):\n",
    "                    prob += pulp.lpSum(y[m, i, l] for m in M_servers_L) <= n_il[i][l-1]\n",
    "                # 否则,添加约束:\n",
    "                # 每个城市每个等级分配的服务员数量等于该等级任务数量\n",
    "                else:\n",
    "                    prob += pulp.lpSum(y[m, i, l] for m in M_servers_L) == n_il[i][l-1]\n",
    "\n",
    "        # 求解问题\n",
    "        prob.solve()\n",
    "\n",
    "        \n",
    "        # 遍历每个服务员、城市和等级\n",
    "        for m in M_servers_L:\n",
    "            for i in I_citys_L:\n",
    "                for l in L_set:\n",
    "                    # 初始化一个临时列表,用于存储分配结果\n",
    "                    Y_sub_set = []\n",
    "                    # 如果当前服务员被分配到某个城市某个等级\n",
    "                    if pulp.value(y[m, i, l]) == 1:\n",
    "                        # 将当前服务员、城市(城市编号从1开始)和等级的分配结果存储到Y_best_allocation字典中\n",
    "                        Y_best_allocation[m] = (m, i+1, l)\n",
    "                        # 中断内层循环,开始处理下一个服务员\n",
    "                        break\n",
    "\n",
    "        # 更新总收益\n",
    "        R_total_reward += pulp.value(prob.objective)\n",
    "\n",
    "        # 提取结果\n",
    "        # Y_L = [(i, l) for m in M_servers_L for i in I_citys_L for l in L_set if y[m, i, l].reward() == 1]\n",
    "        # Y_set.append(Y_L)\n",
    "\n",
    "    \n",
    "    # 步骤3:安排放假的员工\n",
    "    for m in range(M_servers):\n",
    "        if servers_info[m][1] == 0:\n",
    "            Y_best_allocation[m] = (m, H_home_of_server[m]+1, 0)  # 城市编号从1开始\n",
    "    # # 步骤3:计算总收益\n",
    "    # R_total_reward = sum(prob.objective.reward() for L in L_set) - C_h\n",
    "    #     Y_set = []\n",
    "        # 解析结果\n",
    "\n",
    "\n",
    "    return Y_best_allocation, R_total_reward\n",
    "\n",
    "\n",
    "def func6(S, mathcal_L, mathscr_L, N_1, N_2):\n",
    "    \"\"\"\n",
    "    生成状态 S 的决策空间 A,满足约束条件。\n",
    "\n",
    "    参数:\n",
    "    S (tuple): 当前状态,包含任务矩阵和服务员信息。\n",
    "    mathcal_L (list): 分类后的等级列表。\n",
    "    mathscr_L (list): 所有服务员的等级列表\n",
    "    N_1 (list): 每个等级的服务员数量。\n",
    "    N_2 (list): 每个等级的任务数量。\n",
    "\n",
    "    返回:\n",
    "    list: 决策空间 A,包含每个服务员的所有可能决策。\n",
    "    \"\"\"\n",
    "    n_il, servers_info = S\n",
    "    M_servers = len(servers_info)  # 服务员数量\n",
    "    I_citys = len(n_il)          # 城市数量\n",
    "\n",
    "    A = [[] for _ in range(M_servers)]  # 初始化决策空间 A\n",
    "    A_mil =  A\n",
    "\n",
    "    # 对每个服务员生成可能的决策\n",
    "    for m in range(M_servers):\n",
    "        if servers_info[m][1] == 0:  # 服务员 m 放假\n",
    "            A[m].append((servers_info[m][0], 0))  # 放假的服务员只有一个决策,即回家\n",
    "        else:  # 服务员 m 工作\n",
    "            for L_set in mathcal_L:  # 遍历每个等级类\n",
    "                if mathscr_L[m] in L_set:  # 如果服务员 m 的等级属于当前等级类\n",
    "                    for i in range(I_citys):  # 遍历每个城市\n",
    "                        for l in L_set:  # 遍历当前等级类的每个等级\n",
    "                            if l >= mathscr_L[m]:  # 如果当前等级不低于服务员 m 的等级\n",
    "                                if (sum(N_1[l-1] for l in L_set) <= sum(N_2[l-1] for l in L_set) and \n",
    "                                    sum(1 for a in A[m] if a[0] == i+1) < sum(n_il[i][l-1] for l in L_set)) or \\\n",
    "                                   (sum(N_1[l-1] for l in L_set) > sum(N_2[l-1] for l in L_set) and\n",
    "                                    sum(1 for a in A[m] if a[0] == i+1 and a[1] == l) < n_il[i][l-1]):\n",
    "                                    # 如果满足约束条件,则将决策添加到服务员 m 的决策空间中\n",
    "                                    A[m].append((i+1, l))  # 城市编号从1开始\n",
    "\n",
    "    return A\n",
    "\n",
    "def func7(T, x_max_task_num, lambda_il):\n",
    "    # 生成了每日新到达的任务?\n",
    "    # T: 表示时间周期，例如天数\n",
    "    # x_max_task_num: 矩阵元素的最大取值\n",
    "    # lambda_il: 泊松分布的率参数矩阵 (I_citys x_max_task_num L_levels)\n",
    "\n",
    "    # 获取 lambda_il 的维度为 I_citys 和 L_levels\n",
    "    I_citys, L_levels = lambda_il.shape\n",
    "\n",
    "    # 初始化三维数组\n",
    "    arriving_taskss = np.zeros((T, I_citys, L_levels), dtype=int)\n",
    "    \n",
    "    # 生成每个时间步的 I_citys x_max_task_num L_levels 矩阵\n",
    "    for t in range(T):\n",
    "        for i in range(I_citys):\n",
    "            for l in range(L_levels):\n",
    "                # 使用泊松分布生成矩阵元素\n",
    "                arriving_taskss[t, i, l] = min(np.random.poisson(lambda_il[i, l]), x_max_task_num)\n",
    "    \n",
    "    return arriving_taskss\n",
    "\n",
    "func1 = generate_state\n",
    "func2 = aggreg_state\n",
    "\n",
    "func3 = func3\n",
    "func4 = func4\n",
    "func5 = func5\n",
    "func6 = func6\n",
    "func7 = func7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 2, 1],\n",
      "       [0, 0, 1, 0, 0],\n",
      "       [1, 0, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 0],\n",
      "       [1, 1, 0, 0, 0],\n",
      "       [1, 0, 0, 1, 1],\n",
      "       [1, 0, 0, 2, 1],\n",
      "       [0, 0, 0, 0, 2],\n",
      "       [0, 0, 2, 1, 0],\n",
      "       [0, 2, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 0],\n",
      "       [0, 0, 2, 0, 0],\n",
      "       [0, 0, 0, 1, 3],\n",
      "       [1, 0, 0, 0, 0],\n",
      "       [1, 3, 0, 1, 2],\n",
      "       [0, 0, 1, 0, 0],\n",
      "       [1, 1, 2, 2, 1],\n",
      "       [0, 0, 4, 0, 2],\n",
      "       [2, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0],\n",
      "       [1, 0, 1, 0, 0],\n",
      "       [0, 0, 1, 3, 0],\n",
      "       [1, 0, 0, 0, 2],\n",
      "       [0, 0, 0, 0, 0],\n",
      "       [1, 0, 2, 0, 0]]), [(26, 3), (5, 2), (25, 5), (14, 4), (5, 3), (6, 6), (14, 0), (20, 5), (12, 4), (26, 1), (7, 4), (1, 3), (5, 5), (4, 6), (10, 1), (5, 5), (2, 3), (10, 3), (19, 1), (1, 1), (13, 4), (16, 3), (3, 6), (17, 1), (20, 3), (18, 3), (1, 2), (19, 0), (5, 2), (3, 3), (1, 0), (11, 1), (13, 3), (14, 3), (19, 1), (3, 1), (7, 6), (26, 0), (26, 4), (26, 1)])\n"
     ]
    }
   ],
   "source": [
    "I_citys = 26\n",
    "L_levels = 5\n",
    "W_workdays = 6\n",
    "M_servers = 40\n",
    "x_max_task_num = 2\n",
    "np.random.seed(42)\n",
    "H_home_of_server = [1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,1,2,3,4,5,6,7,8,9,10,]\n",
    "lambd = np.random.rand(I_citys, L_levels)\n",
    "func1 = generate_state\n",
    "S_t = func1(I_citys, L_levels, W_workdays, M_servers, x_max_task_num, H_home_of_server, lambd)\n",
    "print(S_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_cluster_num=3\n",
    "X=3\n",
    "M_servers=40\n",
    "I_citys=26\n",
    "L_levels=5\n",
    "barS=aggreg_state(S_t, Z_cluster_num, X, M_servers, I_citys, L_levels)\n",
    "S = S_t\n",
    "# (位置i_m, 放假天数w_m)\n",
    "rest_day_L = [server_restday[1] for server_restday in S_t[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,), (2,), (3,), (4,), (5,)] [11, 6, 8, 4, 7] [14, 8, 16, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "S = S_t\n",
    "# (位置i_m, 放假天数w_m)\n",
    "mathscr_L = [1,1,1,2,3,4,5,2,3,5, 1,1,1,2,3,4,5,2,3,5,1,1,1,2,3,4,5,2,3,5,1,1,1,2,3,4,5,2,3,5,]\n",
    "mathcal_L, N_1, N_2 = func4(S, mathscr_L)\n",
    "print(mathcal_L, N_1, N_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = S_t\n",
    "n_il, servers_info = S\n",
    "L_server = mathscr_L\n",
    "H_home_of_server = H_home_of_server\n",
    "r1_reward  = [0, 3500, 3000, 2500, 2000, 1500]\n",
    "# 假设 I_citys 是城市的数量，这个值应该根据你的具体情况来设置\n",
    "I_citys = len(n_il)  # 以 n_il 变量中元素的数量来确定城市数量\n",
    "\n",
    "# 生成随机的成本矩阵 c1_city_cost\n",
    "random.seed(42)\n",
    "c1_city_cost = [[0 if i == j else random.randint(100, 500) for j in range(I_citys)] for i in range(I_citys)]\n",
    "c2 = 100\n",
    "A =func3(S, L_server, H_home_of_server, r1_reward, c1_city_cost, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 26, 1),\n",
       " (1, 15, 1),\n",
       " (2, 4, 1),\n",
       " (3, 11, 2),\n",
       " (4, 13, 3),\n",
       " (5, 23, 4),\n",
       " (6, 8, 0),\n",
       " (7, 11, 2),\n",
       " (8, 10, 3),\n",
       " (9, 14, 5),\n",
       " (10, 7, 1),\n",
       " (11, 18, 1),\n",
       " (12, 5, 1),\n",
       " (13, 6, 2),\n",
       " (14, 10, 3),\n",
       " (15, 8, 4),\n",
       " (16, 2, 5),\n",
       " (17, 16, 2),\n",
       " (18, 19, 3),\n",
       " (19, 19, 5),\n",
       " (20, 20, 1),\n",
       " (21, 16, 1),\n",
       " (22, 6, 1),\n",
       " (23, 18, 2),\n",
       " (24, 22, 3),\n",
       " (25, 18, 4),\n",
       " (26, 1, 5),\n",
       " (27, 9, 0),\n",
       " (28, 13, 3),\n",
       " (29, 19, 5),\n",
       " (30, 2, 0),\n",
       " (31, 8, 1),\n",
       " (32, 20, 1),\n",
       " (33, 16, 2),\n",
       " (34, 19, 3),\n",
       " (35, 10, 4),\n",
       " (36, 7, 5),\n",
       " (37, 9, 0),\n",
       " (38, 26, 3),\n",
       " (39, 14, 5)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_best_allocation, R_total_reward = func5(S,mathcal_L, mathscr_L, N_1, N_2, H_home_of_server, r1_reward, c1_city_cost, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 26, 1),\n",
       " (1, 5, 1),\n",
       " (2, 4, 1),\n",
       " (3, 16, 2),\n",
       " (4, 13, 3),\n",
       " (5, 23, 4),\n",
       " (6, 8, 0),\n",
       " (7, 11, 2),\n",
       " (8, 10, 3),\n",
       " (9, 14, 5),\n",
       " (10, 7, 1),\n",
       " (11, 18, 1),\n",
       " (12, 15, 1),\n",
       " (13, 6, 2),\n",
       " (14, 10, 3),\n",
       " (15, 8, 4),\n",
       " (16, 2, 5),\n",
       " (17, 16, 2),\n",
       " (18, 19, 3),\n",
       " (19, 19, 5),\n",
       " (20, 20, 1),\n",
       " (21, 16, 1),\n",
       " (22, 6, 1),\n",
       " (23, 18, 2),\n",
       " (24, 22, 3),\n",
       " (25, 18, 4),\n",
       " (26, 1, 5),\n",
       " (27, 9, 0),\n",
       " (28, 13, 3),\n",
       " (29, 19, 5),\n",
       " (30, 2, 0),\n",
       " (31, 8, 1),\n",
       " (32, 20, 1),\n",
       " (33, 11, 2),\n",
       " (34, 19, 3),\n",
       " (35, 10, 4),\n",
       " (36, 7, 5),\n",
       " (37, 9, 0),\n",
       " (38, 26, 3),\n",
       " (39, 14, 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_best_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_all = func6(S, mathcal_L, mathscr_L, N_1, N_2)\n",
    "len(A_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 [13, 13, 13, 5, 9, 10, 1, 5, 9, 10, 13, 13, 13, 5, 9, 10, 10, 5, 9, 10, 13, 13, 13, 5, 9, 10, 10, 1, 9, 10, 1, 13, 13, 5, 9, 10, 10, 1, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "A_all_lens = [len(A) for A in A_all]\n",
    "print(len(A_all), A_all_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例参数\n",
    "T = 7  # 时间步数量\n",
    "x_max_task_num = 3  # 最大值\n",
    "I_citys = 26  # 城市数量\n",
    "L_levels = 5  # 等级数量\n",
    "\n",
    "np.random.seed(42)\n",
    "lambda_il = np.random.rand(I_citys, L_levels)  # 生成率参数矩阵\n",
    "\n",
    "# 生成arriving_tasks_i\n",
    "arriving_tasks_for_T = func7(T, x_max_task_num, lambda_il)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # def func5_with_V(self, t, S, mathcal_L, mathscr_L, N_1, N_2, H_home_of_server, r1_reward, c1_city_cost, c2, V, Xi_arriving_tasks):\n",
    "    #     \"\"\"\n",
    "    #     生成状态 S 到决策 Y_best_allocation 的函数,通过解决线性规划问题来最大化收益 R_total_reward(S, Y_best_allocation) + V(t+1, S').\n",
    "\n",
    "    #     参数:\n",
    "    #     t (int): 当前时间步。\n",
    "    #     S (tuple): 当前状态,包含任务矩阵和服务员信息。\n",
    "    #     mathcal_L (list): 分类后的等级列表。\n",
    "    #     mathscr_L (list): 所有服务员的等级列表\n",
    "    #     N_1 (list): 每个等级的服务员数量。\n",
    "    #     N_2 (list): 每个等级的任务数量。\n",
    "    #     H_home_of_server (list): 服务员的家位置列表。\n",
    "    #     r1_reward (list): 每个等级的收益列表。\n",
    "    #     c1_city_cost (list of list): I×I 的成本矩阵。\n",
    "    #     c2 (float): 常数成本。\n",
    "    #     V (ValueFunction): 值函数对象。\n",
    "\n",
    "    #     返回:\n",
    "    #     list: 最优决策 Y_best_allocation,包含每个服务员的位置和等级。\n",
    "    #     R_total_reward, 总收益\n",
    "    #     \"\"\"\n",
    "    #     n_il, servers_info = S\n",
    "    #     M_servers = len(servers_info)  # 服务员数量\n",
    "    #     I_citys = len(n_il)          # 城市数量\n",
    "    #     L_max = [max(l) for l in mathcal_L]  # 最大等级\n",
    "\n",
    "    #     # 步骤1:安排放假的员工回家\n",
    "    #     C_h = sum(c1_city_cost[servers_info[m][0]-1][H_home_of_server[m]] for m in range(M_servers) if servers_info[m][1] == 0)\n",
    "        \n",
    "    #     R_total_reward = -C_h  # 初始化总收益为负的回家成本\n",
    "\n",
    "    #     Y_best_allocation = [None] * M_servers  # 初始化最优决策 Y_best_allocation\n",
    "        \n",
    "    #     # 步骤2:对每个等级类独立进行员工分配\n",
    "    #     for L_set, l_max_L in zip(mathcal_L, L_max):\n",
    "    #         M_servers_L = [m for m in range(M_servers) if servers_info[m][1] > 0 and mathscr_L[m] in L_set]  # 该等级类下工作的员工集合 工作：(servers_info[m][1] > 0) 该等级类for m in range(M_servers) mathscr_L[m] in L_set \n",
    "    #         I_citys_L = [i for i in range(I_citys) if any(n_il[i][l-1] > 0 for l in L_set)]  # 该等级类下有任务需求的城市集合\n",
    "\n",
    "    #         # 创建问题实例\n",
    "    #         prob = pulp.LpProblem(f\"Optimal_Server_Assignment_Level_{L_set}\", pulp.LpMaximize)\n",
    "\n",
    "    #         # 定义决策变量 y_{mil} 为二元变量\n",
    "    #         y = pulp.LpVariable.dicts(\"y\", \n",
    "    #                                 ((m, i, l) for m in M_servers_L for i in I_citys_L for l in L_set), \n",
    "    #                                 cat=pulp.LpBinary)\n",
    "    #         # 定义一个辅助函数,计算下一个状态的价值函数\n",
    "    #         def calculate_next_state_reward(arriving_tasks):\n",
    "    #             \"\"\"\n",
    "    #             计算下一个状态的价值函数\n",
    "    #             Args:\n",
    "    #                 arriving_tasks (list): 当前时间步到达的任务\n",
    "    #             Returns:\n",
    "    #                 float: 下一个状态的价值函数\n",
    "    #             \"\"\"\n",
    "    #             # 计算下一个状态\n",
    "    #             new_state = tuple(n_il[i][l-1] - pulp.lpSum(y[m, i, l] for m in M_servers_L) + arriving_tasks[i][l-1] \n",
    "    #                             for i in range(I_citys) for l in L_set)\n",
    "    #             # 压缩状态\n",
    "    #             S_bar_next = func2(new_state, self.Z_cluster_num, self.X, M_servers, I_citys, self.L_levels)\n",
    "    #             # 更新价值函数的计数\n",
    "    #             count = V.get_count(t+1, S_bar_next)\n",
    "    #             # 返回下一个状态的价值函数\n",
    "    #             return V.get(t+1, S_bar_next) * (1 - 1 / count)\n",
    "\n",
    "    #         # 等一下这里不太对，这里是对每个集合内的\n",
    "    #         # 目标函数\n",
    "    #         # 定义一个辅助变量,存储服务员分配的收益和城市成本\n",
    "    #         reward_cost_expr = pulp.lpSum(\n",
    "    #             r1_reward[l1-1] * y[m1, i1, l1] - c1_city_cost[servers_info[m1][0]-1][i1] * y[m1, i1, l1] \n",
    "    #             for m1 in M_servers_L for i1 in I_citys_L for l1 in L_set\n",
    "    #         )\n",
    "\n",
    "    #         # 根据等级类型选择目标函数\n",
    "    #         # 如果每个等级的服务员数量小于等于任务数量,则最大化收益减去未分配任务的惩罚,加上后续时间步的预期价值\n",
    "    #         if sum(N_1[l-1] for l in L_set) <= sum(N_2[l-1] for l in L_set):\n",
    "    #             prob += reward_cost_expr \\\n",
    "    #                 - c2 * pulp.lpSum(n_il[i][l-1] - pulp.lpSum(y[m, i, l] for m in M_servers_L) \n",
    "    #                                     for i in I_citys_L for l in L_set) \\\n",
    "    #                 + pulp.lpSum(calculate_next_state_reward(arriving_tasks) \n",
    "    #                                 for arriving_tasks in Xi_arriving_tasks)\n",
    "    #         # 否则,只最大化收益,加上后续时间步的预期价值\n",
    "    #         else:\n",
    "    #             prob += reward_cost_expr \\\n",
    "    #                 + pulp.lpSum(calculate_next_state_reward(arriving_tasks) \n",
    "    #                                 for arriving_tasks in Xi_arriving_tasks)\n",
    "\n",
    "    #         # 添加约束\n",
    "    #         for m in M_servers_L: \n",
    "    #             # 每个工作中的服务员 m,要求其被分配到城市 i 提供的服务等级 l 必须不低于他自身的服务等级 L_server[m]\n",
    "    #             # 且只能被分配到一个城市提供一种等级的服务。\n",
    "    #             prob += pulp.lpSum(y[m, i, l] for i in I_citys_L for l in L_set if l >= mathscr_L[m]) == 1\n",
    "\n",
    "    #         # 添加约束条件\n",
    "    #         for i in I_citys_L:\n",
    "    #             for l in L_set:\n",
    "    #                 # 如果每个等级的服务员数量小于等于任务数量,则添加约束:\n",
    "    #                 # 每个城市每个等级分配的服务员数量小于等于该等级任务数量\n",
    "    #                 if sum(N_1[l-1] for l in L_set) <= sum(N_2[l-1] for l in L_set):\n",
    "    #                     prob += pulp.lpSum(y[m, i, l] for m in M_servers_L) <= n_il[i][l-1]\n",
    "    #                 # 否则,添加约束:\n",
    "    #                 # 每个城市每个等级分配的服务员数量等于该等级任务数量\n",
    "    #                 else:\n",
    "    #                     prob += pulp.lpSum(y[m, i, l] for m in M_servers_L) == n_il[i][l-1]\n",
    "\n",
    "    #         # 求解问题\n",
    "    #         prob.solve()\n",
    "\n",
    "    #         # 记录最优分配结果\n",
    "    #         for m in M_servers_L:\n",
    "    #             for i in I_citys_L:\n",
    "    #                 for l in L_set:\n",
    "    #                     # 如果当前服务员被分配到某个城市某个等级\n",
    "    #                     if pulp.value(y[m, i, l]) == 1:\n",
    "    #                         # 将当前服务员、城市(城市编号从1开始)和等级的分配结果存储到Y_best_allocation字典中\n",
    "    #                         Y_best_allocation[m] = (m, i+1, l)\n",
    "    #                         # 中断内层循环,开始处理下一个服务员\n",
    "    #                         break\n",
    "\n",
    "    #         # 计算后续时间步的预期价值\n",
    "    #         for arriving_tasks in Xi_arriving_tasks:\n",
    "    #             # 计算下一个状态的价值函数\n",
    "    #             R_total_reward += calculate_next_state_reward(arriving_tasks)\n",
    "\n",
    "    #         # 步骤3:安排放假的员工\n",
    "    #         for m in range(M_servers):\n",
    "    #             if servers_info[m][1] == 0:\n",
    "    #                 Y_best_allocation[m] = (m, H_home_of_server[m]+1, 0)  # 城市编号从1开始\n",
    "\n",
    "    #         return Y_best_allocation, R_total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class ValueFunction:\n",
    "    def __init__(self, T=7, file_path=\"./reward_function.pkl\"):\n",
    "        self.rewards = {key: {} for key in range(T + 1)}  # 用字典存储状态-价值对\n",
    "        self.count = {key: {} for key in range(T + 1)}  # 用字典存储状态出现次数\n",
    "        self.policies = {key: {} for key in range(T + 1)}  # 用字典存储状态-最优决策对\n",
    "        self.file_path = file_path\n",
    "\n",
    "\n",
    "    def set_reward(self, t, S_t, reward):\n",
    "        \"\"\"\n",
    "        存储阶段t和状态state的价值reward\n",
    "        \"\"\"\n",
    "        S_t_str = process_state(S_t)\n",
    "        self.rewards[t][S_t_str] = reward\n",
    "\n",
    "    \n",
    "    def get_reward(self, t, S_t):\n",
    "        \"\"\"\n",
    "        获取阶段t和状态state的价值,如果状态不存在,则返回默认值0\n",
    "        \"\"\"\n",
    "        S_t_str = process_state(S_t)\n",
    "        if S_t_str not in self.rewards[t]:\n",
    "            self.rewards[t][S_t_str] = 0\n",
    "        else:\n",
    "            if self.rewards[t][S_t_str] > 0:\n",
    "                print(f\"rewards 大于 0 get rewards! {t=} {self.rewards[t][S_t_str]=} \")\n",
    "        \n",
    "        return self.rewards[t][S_t_str]\n",
    "\n",
    "    def update_reward(self, t, S_t, current_reward):\n",
    "        updated = False\n",
    "        last_reward = self.get_reward(t, S_t)\n",
    "        if last_reward >= current_reward:\n",
    "            pass\n",
    "        else:\n",
    "            self.set_reward(t, S_t, current_reward)\n",
    "            updated = True\n",
    "        return updated\n",
    "        \n",
    "    \n",
    "    def set_policy(self, t, S_t, policy_t_s):\n",
    "        \"\"\"\n",
    "        设置阶段t和状态state的最优决策policy_t_s\n",
    "        \"\"\"\n",
    "        S_t_str = process_state(S_t)\n",
    "        self.policies[t][S_t_str] = policy_t_s\n",
    "\n",
    "    def get_policy(self, t, S_t):\n",
    "        \"\"\"\n",
    "        获取阶段t和状态state的最优决策,如果状态不存在或没有最优决策,则返回None\n",
    "        \"\"\"\n",
    "        S_t_str = process_state(S_t)\n",
    "        if S_t_str not in self.policies[t]:\n",
    "            self.policies[t][S_t_str] = None\n",
    "        else:\n",
    "            print(f\"get policy! {t=} {self.policies[t][S_t_str]=} {S_t=}\")\n",
    "        return self.policies[t][S_t_str]\n",
    "\n",
    "    # def update_policy(self, t, S_t, Y_best_allocation, S_t_1, S_last_t_1):\n",
    "    #     S_t_str = process_state(S_t)\n",
    "    #     R_s_t = self.get_reward(t + 1, S_t_1)\n",
    "    #     R_s_t_last = self.get_reward(t + 1, S_last_t_1)\n",
    "    #     if S_t_str not in self.policies[t] or R_s_t > R_s_t_last:\n",
    "    #         self.set_policy(self, t, S_t_str, policy_t_s=Y_best_allocation)\n",
    "            \n",
    "    def get_count(self, t, S_t):\n",
    "        \"\"\"\n",
    "        获取阶段t和状态state的出现次数,如果状态不存在,则返回默认值1 表示当前出现一次\n",
    "        \"\"\"\n",
    "        S_t_str = process_state(S_t)\n",
    "        if S_t_str not in self.count[t]:\n",
    "            self.set_count(t=t, S_t=S_t, count=1)\n",
    "        else:\n",
    "            if self.count[t][S_t_str] > 1:\n",
    "                print(f\"get count! {t} {self.count[t][S_t_str]} {S_t}\")\n",
    "        return self.count[t][S_t_str]\n",
    "\n",
    "    def set_count(self, t, S_t, count):\n",
    "        S_t_str = process_state(S_t)\n",
    "        self.count[t][S_t_str] = count \n",
    "        \n",
    "    def init_or_add_count(self, t, S_t):\n",
    "        count = self.get_count(t, S_t)\n",
    "        return count\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def save_model(self, file_path=None):\n",
    "        \"\"\"\n",
    "        保存 ValueFunction 实例到文件\n",
    "        \"\"\"\n",
    "        if file_path is None:\n",
    "            file_path = self.file_path\n",
    "        if file_path is None:\n",
    "            raise ValueError(\"No file path provided for saving the ValueFunction object.\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        \"\"\"\n",
    "        从文件中加载 ValueFunction 实例\n",
    "        \"\"\"\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "def process_state(S_t):\n",
    "    \"\"\"\n",
    "    将状态转换为NumPy数组，并返回转换后的状态。\n",
    "    \"\"\"\n",
    "    if type(S_t) ==  str:\n",
    "        S_t_str = S_t\n",
    "    else:\n",
    "        S_t_str = str(S_t)\n",
    "    caller_frame = inspect.currentframe().f_back\n",
    "    caller_name = caller_frame.f_code.co_name\n",
    "    # print(f\"{caller_name=} {S_t_str=}\")\n",
    "    return S_t_str\n",
    "\n",
    "class Func8:\n",
    "    \n",
    "    pi = {}  # 初始化策略\n",
    "    I_citys = 1\n",
    "    L_levels = 5\n",
    "    W_workdays = 6\n",
    "    M_servers = 1\n",
    "    x_max_task_num = 2\n",
    "    random.seed(42)\n",
    "    lambd = np.random.rand(I_citys, L_levels)\n",
    "    r1_reward  = [0, 3500, 3000, 2500, 2000, 1500]\n",
    "    c2 = 100\n",
    "\n",
    "    max_iter = 10000\n",
    "    T = 7\n",
    "\n",
    "    Z_cluster_num=3\n",
    "    X=3\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.V = ValueFunction()  # 初始化值函数\n",
    "        self.H_home_of_server = [random.randint(1, self.I_citys) for _ in range(self.M_servers)] # 随机家的位置\n",
    "        self.c1_city_cost = [[0 if i == j else random.randint(100, 500) \n",
    "                              for j in range(self.I_citys)] for i in range(self.I_citys)]\n",
    "        self.lambda_il = np.random.rand(self.I_citys, self.L_levels)  # 生成率参数矩阵\n",
    "        self.arriving_tasks_for_T = func7(self.T, self.x_max_task_num, self.lambda_il)\n",
    "        self.mathscr_L = [random.randint(1, self.L_levels) for _ in range(self.M_servers)]\n",
    "        print(f\"{self.H_home_of_server=} {self.mathscr_L=}\")\n",
    "        \n",
    "    def func3_transfer(self, S_t, L_server, H_home_of_server, r1_reward, c1_city_cost, c2, t, Xi_arriving_tasks):\n",
    "\n",
    "        n_il, servers_info = S_t\n",
    "        M_servers = self.M_servers  # 服务员数量\n",
    "        I_citys = self.I_citys          # 城市数量\n",
    "        L_max = max(L_server)        # 最大等级\n",
    "        n_next = [[pulp.LpVariable(f\"x_{i}_{j}\",cat='Integer') for j in range(L_max+1)] for i in range(I_citys)]\n",
    "        ser_info_1= [pulp.LpVariable(f\"u_{i}\",cat='Integer') for i in range(M_servers)]\n",
    "        ser_info_2 = [0] * M_servers\n",
    "        \n",
    "        # 创建问题实例\n",
    "        prob = pulp.LpProblem(\"Optimal_Server_Assignment\", pulp.LpMaximize)\n",
    "\n",
    "        # 定义决策变量 y_{mil} 为二元变量\n",
    "        y = pulp.LpVariable.dicts(\"y\", \n",
    "                                ((m, i, l) for m in range(M_servers) for i in range(I_citys) for l in range(L_max+1)), \n",
    "                                cat=pulp.LpBinary)\n",
    "        \n",
    "        for i in range(I_citys):\n",
    "            for l in range(L_max):\n",
    "                n_next[i][l] = n_il[i][l] - pulp.lpSum(y[m,i,l+1] for m in range(M_servers))+Xi_arriving_tasks\n",
    "\n",
    "        # 每个服务员 m 最多只能被分配到一个城市的一个等级的任务。\n",
    "        for m in range(M_servers):\n",
    "            ser_info_1[m] = pulp.lpSum(i * pulp.lpSum(y[m,i,l] for l in range(0,L_max+1)) for i in range(I_citys))\n",
    "        \n",
    "        for m in range(M_servers):\n",
    "            if servers_info[m][1] == 0:\n",
    "                ser_info_2[m] =self.W_workdays\n",
    "            else:\n",
    "                ser_info_2[m] = servers_info[m][1]-1\n",
    "        ser_info_next=list(zip(ser_info_1,ser_info_2))\n",
    "        S_next = (n_next, ser_info_next)\n",
    "        \n",
    "        policy_for_S = self.V.get_policy(t, S_t=S_t)\n",
    "        if policy_for_S:\n",
    "            S_next_last = self.transition(S=S_t, Y_best_allocation=policy_for_S, Xi_arriving_tasks=Xi_arriving_tasks)\n",
    "        else:\n",
    "            S_next_last = S_t\n",
    "        reward = self.V.get_reward(t=t, S_t=S_next_last)\n",
    "\n",
    "        prob += pulp.lpSum(\n",
    "            r1_reward[l] * y[m, i, l] - c1_city_cost[servers_info[m][0]-1][i] * y[m, i, l]\n",
    "                        for m in range(M_servers) for i in range(I_citys)\n",
    "                        for l in range(0, L_max+1))\\\n",
    "                            - c2 * pulp.lpSum(n_il[i][l-1] - pulp.lpSum(y[m, i, l] for m in range(M_servers)) for i in range(I_citys)\n",
    "                        for l in range(1, L_max+1)) + reward\n",
    "        \n",
    "\n",
    "        # for m in range(M_servers):\n",
    "        #     prob += pulp.lpSum(y[m, i, l] for i in range(I_citys) for l in range(L_max+1)) == 1\n",
    "\n",
    "\n",
    "        # 分配给所有服务员的所有任务总数不能超过实际的任务总数。\n",
    "        total_tasks = sum(n_il[i][l-1] for i in range(I_citys) for l in range(1, L_max+1))\n",
    "        prob += pulp.lpSum(y[m, i, l] for m in range(M_servers) for i in range(I_citys) for l in range(1, L_max+1)) <= total_tasks\n",
    "        \n",
    "        # 分配的任务总数不能超过可分配的业务员数量 (非休息日)。\n",
    "        available_servers = sum(1 for _, (_, wm) in enumerate(servers_info) if wm > 0) \n",
    "        prob += pulp.lpSum(y[m, i, l] for m in range(M_servers) for i in range(I_citys) for l in range(1, L_max+1)) <= available_servers\n",
    "        \n",
    "        \n",
    "        for m, (im, wm) in enumerate(servers_info):\n",
    "            if wm == 0:\n",
    "                # 休息日服务员必须待在家中\n",
    "                prob += y[m, H_home_of_server[m]-1, 0] == 1\n",
    "            elif wm > 0:\n",
    "                # 服务员必须分配任务，且任务等级必须高于服务员等级\n",
    "                # 1. 允许选择 \"待在家中\" (l = 0)\n",
    "                prob += pulp.lpSum(y[m, i, l] for i in range(I_citys) for l in range(L_max+1) if (l == 0 or (l >= L_server[m] and n_il[i][l-1] > 0))) == 1\n",
    "        \n",
    "        # 每个服务员必须且只能被分配到一个任务，无论是在某个城市执行任务还是在家休息。\n",
    "        for m in range(M_servers):\n",
    "            prob += pulp.lpSum(y[m, i, l] for i in range(I_citys) for l in range(L_max + 1)) == 1\n",
    "\n",
    "        # 限制了服务员只能接受等级不高于自身等级的任务\n",
    "        for m in range(M_servers):\n",
    "            for i in range(I_citys):\n",
    "                for l in range(1, L_max + 1):  \n",
    "                    prob += y[m, i, l] <= (L_server[m] >= l)\n",
    "                \n",
    "        for i in range(I_citys):\n",
    "            for l in range(1, L_max+1):\n",
    "                # 每个城市每个等级分配的服务员数量不能超过任务数量\n",
    "                prob += pulp.lpSum(y[m, i, l] for m in range(M_servers)) <= n_il[i][l-1]\n",
    "\n",
    "        # 求解问题\n",
    "        # 选择求解器，例如：CBC\n",
    "        solver = pulp.PULP_CBC_CMD(msg=False)\n",
    "        prob.solve(solver)\n",
    "\n",
    "        # 解析结果\n",
    "        # 解析结果，初始化一个长度为 M_servers 的列表，每个元素为 None\n",
    "        result = [None] * self.M_servers \n",
    "\n",
    "        # 遍历所有服务员\n",
    "        for m in range(self.M_servers):\n",
    "            # 找到分配给该服务员的任务\n",
    "            allocation = [(i + 1, l) for i in range(self.I_citys) for l in range(L_max + 1) if pulp.value(y[m, i, l]) == 1]\n",
    "            # 如果服务员被分配了任务，则将任务添加到 result 列表中\n",
    "            if allocation:\n",
    "                result[m] = (m, allocation[0][0], allocation[0][1])  # 添加服务员编号 m# 获取目标函数值\n",
    "        # 确保每个城市分配的业务员总量不会超过该城市的任务总量\n",
    "        for i in range(I_citys):\n",
    "            prob += pulp.lpSum(y[m, i, l] for m in range(M_servers) for l in range(1, L_max+1)) <= sum(n_il[i])\n",
    "        # 解析结果\n",
    "        result = [None] * self.M_servers \n",
    "\n",
    "\n",
    "        for m in range(self.M_servers):\n",
    "            # 找到分配给该服务员的城市和任务等级\n",
    "            assigned = False  # 用于标记服务员是否被分配了任务\n",
    "            for i in range(self.I_citys):\n",
    "                for l in range(L_max + 1):\n",
    "                    if pulp.value(y[m, i, l]) == 1:\n",
    "                        result[m] = (m, i + 1, l)  # 添加服务员编号 m\n",
    "                        assigned = True\n",
    "                        # print(f\"{result[m]=}\")\n",
    "                # if assigned:\n",
    "                #     break\n",
    "\n",
    "            # 如果服务员没有被分配任务，则设置其在家\n",
    "            if not assigned:\n",
    "                result[m] = (m, H_home_of_server[m], 0)\n",
    "        # 获取目标函数值\n",
    "        objective_value = pulp.value(prob.objective)\n",
    "\n",
    "        # print(f\"{result=}\")\n",
    "        return result, objective_value\n",
    "\n",
    "\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        带起始探索的强化学习算法,生成最优值函数和最优策略。\n",
    "\n",
    "        返回:\n",
    "        tuple: 包含最优状态、最优值函数和最优策略的元组。\n",
    "        \"\"\"\n",
    "        # 初始化值函数和策略\n",
    "        self.V = ValueFunction()\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            print(f\"-------------迭代次数{i=}--------------\")\n",
    "            # 生成初始状态\n",
    "            S_init = func1(I_citys=self.I_citys,\n",
    "                    L_levels=self.L_levels,\n",
    "                    W_workdays=self.W_workdays,\n",
    "                    M_servers=self.M_servers,\n",
    "                    x_max_task_num=self.x_max_task_num,\n",
    "                    H_home_of_server=self.H_home_of_server,\n",
    "                    lambd=self.lambd)\n",
    "            print(f\"{S_init=}\")\n",
    "            # 初始化当前轨迹:决策，状态，收益\n",
    "            trajectory = []\n",
    "            S_current = S_init\n",
    "            for t in range(self.T):\n",
    "                \n",
    "                # 更新状态出现次数\n",
    "                self.V.init_or_add_count(t=t, S_t=S_current)\n",
    "\n",
    "                Xi_arriving_tasks = self.arriving_tasks_for_T[t]\n",
    "                \n",
    "                \n",
    "                Y_best_allocation, R_total_reward = self.func3_transfer(S_t=S_current,t=t,\n",
    "                                                                    L_server=self.mathscr_L,\n",
    "                                                                    H_home_of_server=self.H_home_of_server,\n",
    "                                                                    r1_reward=self.r1_reward,\n",
    "                                                                    c1_city_cost=self.c1_city_cost,\n",
    "                                                                    c2=self.c2,\n",
    "                                                                    Xi_arriving_tasks=Xi_arriving_tasks)\n",
    "                print(f\"{t=} {len(Y_best_allocation)=} {R_total_reward=} {Y_best_allocation=} {S_current[1]=} {Xi_arriving_tasks=} {S_current[0]=} \")\n",
    "                # 记录轨迹:当前决策，状态，收益\n",
    "                trajectory.append((S_current, Y_best_allocation, R_total_reward))\n",
    "\n",
    "                # 状态转移\n",
    "                S_next = self.transition(S_current, Y_best_allocation, Xi_arriving_tasks)\n",
    "\n",
    "                S_current = S_next\n",
    "                # print(f\"{S_current=}\")\n",
    "\n",
    "            # 逆向计算G和更新V 和保存策略\n",
    "            G = [0] * (self.T + 1)\n",
    "            reward_diff_sum = 0\n",
    "            for t in range(self.T - 1, -1, -1):\n",
    "                Xi_arriving_tasks = self.arriving_tasks_for_T[t]\n",
    "                S, Y_best_allocation, R_total_reward = trajectory[t]\n",
    "                # S_current, Y_best_allocation, R_total_reward \n",
    "                G[t] = G[t + 1] + R_total_reward \n",
    "                count = self.V.get_count(t, S)\n",
    "                reward_s_t= self.V.get_reward(t, S)\n",
    "                reward = reward_s_t + (1 / count) * (G[t] - reward_s_t)\n",
    "                if count > 1:\n",
    "                    print(f\"{count=} {reward=}\")\n",
    "                self.V.set_reward(t=t, S_t=S, reward=reward)\n",
    "                self.V.set_policy(t=t, S_t=S, policy_t_s=Y_best_allocation)\n",
    "\n",
    "                print(f\"{t=} {len(self.V.rewards[t])=}\\t{len(self.V.count[t])=}\\t {len(self.V.policies[t])=}\")\n",
    "                reward_diff_sum += abs(reward_s_t-reward)\n",
    "            \n",
    "            print(f\"{reward_diff_sum=}\")\n",
    "            # 更新策略\n",
    "            \n",
    "            \n",
    "\n",
    "        # 找到最优状态\n",
    "        S_opt = max(self.V.rewards[0], key=self.V.get_reward)\n",
    "\n",
    "        return S_opt, self.V \n",
    "\n",
    "    def transition(self, S, Y_best_allocation, Xi_arriving_tasks):\n",
    "        # print(\"transiting\")\n",
    "        \"\"\"\n",
    "        状态转移函数。\n",
    "\n",
    "        参数:\n",
    "        S (tuple): 当前状态。\n",
    "        Y_best_allocation (list): 当前决策。 # m, i[1,I_citys] l[0,L_max]\n",
    "        Xi_arriving_tasks (np.array): 每个时间段新增任务的数量矩阵。\n",
    "\n",
    "        返回:\n",
    "        tuple: 下一个状态。\n",
    "        \"\"\"\n",
    "        n_il, servers_info = S\n",
    "        task_count = self.count_tasks(Y_best_allocation)\n",
    "        n_il_new = n_il + Xi_arriving_tasks - task_count\n",
    "        servers_info_new = []\n",
    "        M_servers = self.M_servers\n",
    "\n",
    "        for m in range(len(servers_info)):\n",
    "            i, w = servers_info[m]\n",
    "            if w > 0:\n",
    "                servers_info_new.append((Y_best_allocation[m][1], w-1)) # 0 1 2  m i l\n",
    "            else:\n",
    "                servers_info_new.append((self.H_home_of_server[m], self.W_workdays))\n",
    "        return (n_il_new, servers_info_new)\n",
    "\n",
    "    \n",
    "    def count_tasks(self, Y_best_allocation):\n",
    "        \"\"\"\n",
    "        统计决策Y中完成的任务数量。\n",
    "\n",
    "        参数:\n",
    "        Y_best_allocation (list): 决策。tuple (m, i, l) # m, i[1,I_citys] l[0,L_max]\n",
    "\n",
    "        返回:\n",
    "        np.array: 每个城市每个等级完成的任务数量。\n",
    "        \"\"\"\n",
    "        # 将 Y_best_allocation 转换为 NumPy 数组\n",
    "        Y_array = np.array(Y_best_allocation)\n",
    "\n",
    "        # 从 Y_array 中提取城市和等级信息\n",
    "        cities = Y_array[:, 1] - 1  # 城市索引从 0 开始\n",
    "        levels = Y_array[:, 2]  # 等级索引从 0 开始\n",
    "\n",
    "        # 创建一个零数组，形状为 (self.I_citys, self.L_levels)\n",
    "        counts = np.zeros((self.I_citys, self.L_levels), dtype=int)\n",
    "\n",
    "        # 统计任务数量\n",
    "        for i in range(len(Y_best_allocation)):\n",
    "            city = cities[i]\n",
    "            level = levels[i]\n",
    "            if level!=0: # 去除0 表示 业务员不分配的情况\n",
    "                counts[city, level-1] += 1\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.H_home_of_server=[1] self.mathscr_L=[1]\n",
      "-------------迭代次数i=0--------------\n",
      "S_init=(array([[1, 0, 0, 0, 0]]), [(1, 1)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[1, 0, 0, 0, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=-200.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 0, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 2, 0]]) \n",
      "t=6 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "t=5 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "t=4 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "t=3 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "t=2 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "t=1 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "t=0 len(self.V.rewards[t])=2\tlen(self.V.count[t])=1\t len(self.V.policies[t])=1\n",
      "reward_diff_sum=44800.0\n",
      "-------------迭代次数i=1--------------\n",
      "S_init=(array([[0, 0, 2, 0, 1]]), [(1, 0)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 2, 0, 1]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 3, 0, 1]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 3, 0, 1]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 3, 0, 1]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 3, 0, 1]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 3, 0, 1]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 5, 2, 1]]) \n",
      "t=6 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "t=5 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "t=4 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "t=3 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "t=2 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "t=1 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "t=0 len(self.V.rewards[t])=3\tlen(self.V.count[t])=2\t len(self.V.policies[t])=2\n",
      "reward_diff_sum=34800.0\n",
      "-------------迭代次数i=2--------------\n",
      "S_init=(array([[0, 1, 0, 1, 0]]), [(1, 2)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 1, 0, 1, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 1, 1, 1, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=-100.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 1, 1, 1, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[1, 1, 1, 1, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 1, 1, 1, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 1, 1, 1, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 2, 3, 3, 0]]) \n",
      "t=6 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "t=5 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "t=4 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "t=3 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "t=2 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "t=1 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "t=0 len(self.V.rewards[t])=4\tlen(self.V.count[t])=3\t len(self.V.policies[t])=3\n",
      "reward_diff_sum=38000.0\n",
      "-------------迭代次数i=3--------------\n",
      "S_init=(array([[0, 0, 0, 1, 0]]), [(1, 3)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 1, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 1, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 1, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 1, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 3, 0]]) \n",
      "t=6 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=5 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=4 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=3 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=2 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=1 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=0 len(self.V.rewards[t])=5\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "reward_diff_sum=34800.0\n",
      "-------------迭代次数i=4--------------\n",
      "S_init=(array([[0, 0, 2, 0, 1]]), [(1, 0)])\n",
      "get policy! t=0 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[0, 0, 2, 0, 1]]), [(1, 0)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 2, 0, 1]]) \n",
      "get policy! t=1 self.policies[t][S_t_str]=[(0, 1, 1)] S_t=(array([[2, 0, 3, 0, 1]]), [(1, 6)])\n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 3, 0, 1]]) \n",
      "get policy! t=2 self.policies[t][S_t_str]=[(0, 1, 1)] S_t=(array([[1, 0, 3, 0, 1]]), [(1, 5)])\n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 3, 0, 1]]) \n",
      "get policy! t=3 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[0, 0, 3, 0, 1]]), [(1, 4)])\n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 3, 0, 1]]) \n",
      "get policy! t=4 self.policies[t][S_t_str]=[(0, 1, 1)] S_t=(array([[1, 0, 3, 0, 1]]), [(1, 3)])\n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 3, 0, 1]]) \n",
      "get policy! t=5 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[0, 0, 3, 0, 1]]), [(1, 2)])\n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 3, 0, 1]]) \n",
      "get policy! t=6 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[0, 1, 5, 2, 1]]), [(1, 1)])\n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 5, 2, 1]]) \n",
      "t=6 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "t=5 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "rewards 大于 0 get rewards! t=4 self.rewards[t][S_t_str]=3500.0 \n",
      "t=4 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "rewards 大于 0 get rewards! t=3 self.rewards[t][S_t_str]=3500.0 \n",
      "t=3 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "rewards 大于 0 get rewards! t=2 self.rewards[t][S_t_str]=7000.0 \n",
      "t=2 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "rewards 大于 0 get rewards! t=1 self.rewards[t][S_t_str]=10400.0 \n",
      "t=1 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "rewards 大于 0 get rewards! t=0 self.rewards[t][S_t_str]=10400.0 \n",
      "t=0 len(self.V.rewards[t])=6\tlen(self.V.count[t])=4\t len(self.V.policies[t])=4\n",
      "reward_diff_sum=0.0\n",
      "-------------迭代次数i=5--------------\n",
      "S_init=(array([[0, 0, 0, 0, 1]]), [(1, 4)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 0, 1]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 1]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 1]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 0, 1]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=-100.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 1]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[1, 0, 1, 0, 1]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 2, 1]]) \n",
      "t=6 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "t=5 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "t=4 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "t=3 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "t=2 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "t=1 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "t=0 len(self.V.rewards[t])=7\tlen(self.V.count[t])=5\t len(self.V.policies[t])=5\n",
      "reward_diff_sum=37800.0\n",
      "-------------迭代次数i=6--------------\n",
      "S_init=(array([[1, 0, 0, 0, 0]]), [(1, 6)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[1, 0, 0, 0, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 0, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 0, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 2, 0]]) \n",
      "t=6 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "t=5 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "t=4 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "t=3 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "t=2 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "t=1 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "t=0 len(self.V.rewards[t])=8\tlen(self.V.count[t])=6\t len(self.V.policies[t])=6\n",
      "reward_diff_sum=38300.0\n",
      "-------------迭代次数i=7--------------\n",
      "S_init=(array([[0, 0, 2, 0, 1]]), [(1, 6)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 2, 0, 1]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 3, 0, 1]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 3, 0, 1]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 3, 0, 1]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 3, 0, 1]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 3, 0, 1]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 5, 2, 1]]) \n",
      "t=6 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "t=5 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "t=4 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "t=3 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "t=2 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "t=1 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "t=0 len(self.V.rewards[t])=9\tlen(self.V.count[t])=7\t len(self.V.policies[t])=7\n",
      "reward_diff_sum=34800.0\n",
      "-------------迭代次数i=8--------------\n",
      "S_init=(array([[0, 0, 0, 2, 1]]), [(1, 2)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 2, 1]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 2, 1]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=-100.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 2, 1]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 2, 1]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 2, 1]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 2, 1]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 4, 1]]) \n",
      "t=6 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=5 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=4 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=3 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=2 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=1 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=0 len(self.V.rewards[t])=10\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "reward_diff_sum=38000.0\n",
      "-------------迭代次数i=9--------------\n",
      "S_init=(array([[0, 0, 0, 0, 0]]), [(1, 1)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 0, 0]]) \n",
      "get policy! t=1 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[2, 0, 1, 0, 0]]), [(1, 0)])\n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=-200.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "get policy! t=2 self.policies[t][S_t_str]=[(0, 1, 1)] S_t=(array([[2, 0, 1, 0, 0]]), [(1, 6)])\n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "get policy! t=3 self.policies[t][S_t_str]=[(0, 1, 1)] S_t=(array([[1, 0, 1, 0, 0]]), [(1, 5)])\n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "get policy! t=4 self.policies[t][S_t_str]=[(0, 1, 1)] S_t=(array([[1, 0, 1, 0, 0]]), [(1, 4)])\n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "get policy! t=5 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[0, 0, 1, 0, 0]]), [(1, 3)])\n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 0, 0]]) \n",
      "get policy! t=6 self.policies[t][S_t_str]=[(0, 1, 0)] S_t=(array([[0, 1, 3, 2, 0]]), [(1, 2)])\n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 2, 0]]) \n",
      "t=6 len(self.V.rewards[t])=11\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=5 len(self.V.rewards[t])=11\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "rewards 大于 0 get rewards! t=4 self.rewards[t][S_t_str]=3500.0 \n",
      "t=4 len(self.V.rewards[t])=11\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "rewards 大于 0 get rewards! t=3 self.rewards[t][S_t_str]=7000.0 \n",
      "t=3 len(self.V.rewards[t])=11\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "rewards 大于 0 get rewards! t=2 self.rewards[t][S_t_str]=10400.0 \n",
      "t=2 len(self.V.rewards[t])=11\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "rewards 大于 0 get rewards! t=1 self.rewards[t][S_t_str]=10200.0 \n",
      "t=1 len(self.V.rewards[t])=11\tlen(self.V.count[t])=8\t len(self.V.policies[t])=8\n",
      "t=0 len(self.V.rewards[t])=11\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "reward_diff_sum=10200.0\n",
      "-------------迭代次数i=10--------------\n",
      "S_init=(array([[0, 0, 0, 4, 0]]), [(1, 5)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 4, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 4, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 4, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 4, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 4, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 4, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 6, 0]]) \n",
      "t=6 len(self.V.rewards[t])=12\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "t=5 len(self.V.rewards[t])=12\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "t=4 len(self.V.rewards[t])=12\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "t=3 len(self.V.rewards[t])=12\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "t=2 len(self.V.rewards[t])=12\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "t=1 len(self.V.rewards[t])=12\tlen(self.V.count[t])=9\t len(self.V.policies[t])=9\n",
      "t=0 len(self.V.rewards[t])=12\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "reward_diff_sum=34800.0\n",
      "-------------迭代次数i=11--------------\n",
      "S_init=(array([[0, 0, 1, 2, 0]]), [(1, 1)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 1, 2, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=-200.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 2, 2, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 2, 2, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 2, 2, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 2, 2, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 2, 2, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 4, 4, 0]]) \n",
      "t=6 len(self.V.rewards[t])=13\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "t=5 len(self.V.rewards[t])=13\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "t=4 len(self.V.rewards[t])=13\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "t=3 len(self.V.rewards[t])=13\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "t=2 len(self.V.rewards[t])=13\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "t=1 len(self.V.rewards[t])=13\tlen(self.V.count[t])=10\t len(self.V.policies[t])=10\n",
      "t=0 len(self.V.rewards[t])=13\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "reward_diff_sum=41300.0\n",
      "-------------迭代次数i=12--------------\n",
      "S_init=(array([[5, 0, 2, 0, 2]]), [(1, 3)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=3100.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[5, 0, 2, 0, 2]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3000.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[6, 0, 3, 0, 2]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3100.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[5, 0, 3, 0, 2]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=-400.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[4, 0, 3, 0, 2]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3100.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[5, 0, 3, 0, 2]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=3200.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[4, 0, 3, 0, 2]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=3300.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[3, 1, 5, 2, 2]]) \n",
      "t=6 len(self.V.rewards[t])=14\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "t=5 len(self.V.rewards[t])=14\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "t=4 len(self.V.rewards[t])=14\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "t=3 len(self.V.rewards[t])=14\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "t=2 len(self.V.rewards[t])=14\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "t=1 len(self.V.rewards[t])=14\tlen(self.V.count[t])=11\t len(self.V.policies[t])=11\n",
      "t=0 len(self.V.rewards[t])=14\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "reward_diff_sum=74600.0\n",
      "-------------迭代次数i=13--------------\n",
      "S_init=(array([[0, 0, 0, 1, 2]]), [(1, 0)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 1, 2]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 1, 2]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 2]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 1, 2]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 2]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 1, 2]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 3, 2]]) \n",
      "t=6 len(self.V.rewards[t])=15\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "t=5 len(self.V.rewards[t])=15\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "t=4 len(self.V.rewards[t])=15\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "t=3 len(self.V.rewards[t])=15\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "t=2 len(self.V.rewards[t])=15\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "t=1 len(self.V.rewards[t])=15\tlen(self.V.count[t])=12\t len(self.V.policies[t])=12\n",
      "t=0 len(self.V.rewards[t])=15\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "reward_diff_sum=34800.0\n",
      "-------------迭代次数i=14--------------\n",
      "S_init=(array([[0, 0, 0, 1, 1]]), [(1, 0)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 1, 1]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 1, 1]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 1]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 1, 1]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 1]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 1, 1]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 3, 1]]) \n",
      "t=6 len(self.V.rewards[t])=16\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "t=5 len(self.V.rewards[t])=16\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "t=4 len(self.V.rewards[t])=16\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "t=3 len(self.V.rewards[t])=16\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "t=2 len(self.V.rewards[t])=16\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "t=1 len(self.V.rewards[t])=16\tlen(self.V.count[t])=13\t len(self.V.policies[t])=13\n",
      "t=0 len(self.V.rewards[t])=16\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "reward_diff_sum=34800.0\n",
      "-------------迭代次数i=15--------------\n",
      "S_init=(array([[0, 0, 0, 0, 0]]), [(1, 2)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 0, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=-100.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 0, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 2, 0]]) \n",
      "t=6 len(self.V.rewards[t])=17\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "t=5 len(self.V.rewards[t])=17\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "t=4 len(self.V.rewards[t])=17\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "t=3 len(self.V.rewards[t])=17\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "t=2 len(self.V.rewards[t])=17\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "t=1 len(self.V.rewards[t])=17\tlen(self.V.count[t])=14\t len(self.V.policies[t])=14\n",
      "t=0 len(self.V.rewards[t])=17\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "reward_diff_sum=38000.0\n",
      "-------------迭代次数i=16--------------\n",
      "S_init=(array([[2, 0, 0, 0, 0]]), [(1, 0)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=-200.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[2, 0, 0, 0, 0]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3200.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[4, 0, 1, 0, 0]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3300.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[3, 0, 1, 0, 0]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 0, 0]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[1, 0, 1, 0, 0]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[0, 1, 3, 2, 0]]) \n",
      "t=6 len(self.V.rewards[t])=18\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "t=5 len(self.V.rewards[t])=18\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "t=4 len(self.V.rewards[t])=18\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "t=3 len(self.V.rewards[t])=18\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "t=2 len(self.V.rewards[t])=18\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "t=1 len(self.V.rewards[t])=18\tlen(self.V.count[t])=15\t len(self.V.policies[t])=15\n",
      "t=0 len(self.V.rewards[t])=18\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "reward_diff_sum=67700.0\n",
      "-------------迭代次数i=17--------------\n",
      "S_init=(array([[4, 0, 1, 3, 2]]), [(1, 6)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=3200.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[4, 0, 1, 3, 2]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3100.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[5, 0, 2, 3, 2]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3200.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 4)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[4, 0, 2, 3, 2]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=3300.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[3, 0, 2, 3, 2]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3300.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[3, 0, 2, 3, 2]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[2, 0, 2, 3, 2]]) \n",
      "t=6 len(Y_best_allocation)=1 R_total_reward=-100.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[0, 0, 0, 2, 0]]) S_current[0]=array([[1, 1, 4, 5, 2]]) \n",
      "t=6 len(self.V.rewards[t])=19\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "t=5 len(self.V.rewards[t])=19\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "t=4 len(self.V.rewards[t])=19\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "t=3 len(self.V.rewards[t])=19\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "t=2 len(self.V.rewards[t])=19\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "t=1 len(self.V.rewards[t])=19\tlen(self.V.count[t])=16\t len(self.V.policies[t])=16\n",
      "t=0 len(self.V.rewards[t])=19\tlen(self.V.count[t])=17\t len(self.V.policies[t])=17\n",
      "reward_diff_sum=68600.0\n",
      "-------------迭代次数i=18--------------\n",
      "S_init=(array([[0, 0, 0, 1, 1]]), [(1, 3)])\n",
      "t=0 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 3)] Xi_arriving_tasks=array([[2, 0, 1, 0, 0]]) S_current[0]=array([[0, 0, 0, 1, 1]]) \n",
      "t=1 len(Y_best_allocation)=1 R_total_reward=3400.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 2)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[2, 0, 1, 1, 1]]) \n",
      "t=2 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 1)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 1]]) \n",
      "t=3 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 0)] Xi_arriving_tasks=array([[1, 0, 0, 0, 0]]) S_current[0]=array([[0, 0, 1, 1, 1]]) \n",
      "t=4 len(Y_best_allocation)=1 R_total_reward=3500.0 Y_best_allocation=[(0, 1, 1)] S_current[1]=[(1, 6)] Xi_arriving_tasks=array([[0, 0, 0, 0, 0]]) S_current[0]=array([[1, 0, 1, 1, 1]]) \n",
      "t=5 len(Y_best_allocation)=1 R_total_reward=0.0 Y_best_allocation=[(0, 1, 0)] S_current[1]=[(1, 5)] Xi_arriving_tasks=array([[0, 1, 2, 2, 0]]) S_current[0]=array([[0, 0, 1, 1, 1]]) \n"
     ]
    }
   ],
   "source": [
    "# 创建 Func8 类的实例\n",
    "func8_instance = Func8()\n",
    "\n",
    "S_opt, valueFunction =  func8_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 26, 5) [[[0 0 0 0 1]\n",
      "  [0 0 0 2 1]\n",
      "  [0 0 1 0 0]\n",
      "  [1 0 0 1 0]\n",
      "  [1 0 0 0 0]\n",
      "  [1 1 0 0 0]\n",
      "  [1 0 0 1 1]\n",
      "  [1 0 0 2 1]\n",
      "  [0 0 0 0 2]\n",
      "  [0 0 2 1 0]\n",
      "  [0 2 0 1 0]\n",
      "  [1 0 0 0 0]\n",
      "  [0 0 2 0 0]\n",
      "  [0 0 0 1 3]\n",
      "  [1 0 0 0 0]\n",
      "  [1 3 0 1 2]\n",
      "  [0 0 1 0 0]\n",
      "  [1 1 2 2 1]\n",
      "  [0 0 3 0 2]\n",
      "  [2 0 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  [1 0 1 0 0]\n",
      "  [0 0 1 3 0]\n",
      "  [1 0 0 0 2]\n",
      "  [0 0 0 0 0]\n",
      "  [1 0 2 0 0]]\n",
      "\n",
      " [[0 0 1 0 0]\n",
      "  [0 0 1 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [2 0 0 0 0]\n",
      "  [0 2 0 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 0 2 0]\n",
      "  [1 0 3 0 0]\n",
      "  [0 2 2 2 0]\n",
      "  [1 0 1 0 2]\n",
      "  [0 0 0 0 0]\n",
      "  [2 0 1 0 2]\n",
      "  [0 0 0 1 0]\n",
      "  [1 2 0 1 0]\n",
      "  [3 0 1 0 1]\n",
      "  [0 0 0 0 0]\n",
      "  [0 1 2 0 1]\n",
      "  [1 1 1 1 0]\n",
      "  [1 1 0 1 1]\n",
      "  [0 2 1 0 0]\n",
      "  [0 0 0 3 0]\n",
      "  [2 3 0 2 0]\n",
      "  [0 2 0 0 2]\n",
      "  [0 0 0 0 3]]\n",
      "\n",
      " [[2 0 0 0 1]\n",
      "  [0 0 1 0 1]\n",
      "  [0 2 2 0 0]\n",
      "  [0 0 1 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [1 0 1 1 0]\n",
      "  [0 0 0 2 1]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 2 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 1 0 0]\n",
      "  [1 0 0 0 2]\n",
      "  [0 1 0 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 0 1 0]\n",
      "  [0 1 0 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 1 0 1 1]\n",
      "  [0 0 0 1 0]\n",
      "  [1 0 0 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 0 1 0 1]\n",
      "  [1 0 2 1 1]\n",
      "  [0 0 0 0 0]\n",
      "  [1 1 0 0 0]\n",
      "  [0 0 0 0 0]]\n",
      "\n",
      " [[1 2 0 0 0]\n",
      "  [1 1 2 0 1]\n",
      "  [0 0 1 0 0]\n",
      "  [0 0 0 0 1]\n",
      "  [1 0 0 0 1]\n",
      "  [0 1 0 2 0]\n",
      "  [1 0 0 1 2]\n",
      "  [0 0 0 0 0]\n",
      "  [0 1 0 0 1]\n",
      "  [0 1 0 1 0]\n",
      "  [1 0 0 0 1]\n",
      "  [1 1 0 0 0]\n",
      "  [0 2 3 0 0]\n",
      "  [0 0 2 0 2]\n",
      "  [2 0 0 0 0]\n",
      "  [1 0 0 0 1]\n",
      "  [0 1 0 0 0]\n",
      "  [0 1 1 1 0]\n",
      "  [0 0 1 0 0]\n",
      "  [1 1 1 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 1 0 0]\n",
      "  [0 0 2 0 1]\n",
      "  [0 0 0 1 0]\n",
      "  [0 1 0 0 0]\n",
      "  [1 0 0 0 2]]\n",
      "\n",
      " [[0 0 0 1 0]\n",
      "  [0 0 0 1 2]\n",
      "  [0 1 0 0 0]\n",
      "  [0 0 0 2 1]\n",
      "  [2 0 0 0 0]\n",
      "  [0 0 2 0 0]\n",
      "  [0 1 0 0 1]\n",
      "  [1 0 0 2 0]\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 1 1 0]\n",
      "  [0 0 1 1 2]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 1 0]\n",
      "  [1 0 0 0 3]\n",
      "  [1 0 0 0 1]\n",
      "  [0 2 0 0 0]\n",
      "  [1 0 0 1 0]\n",
      "  [0 1 1 0 1]\n",
      "  [2 0 2 1 1]\n",
      "  [0 0 1 0 0]\n",
      "  [0 1 0 1 1]\n",
      "  [0 1 1 1 1]\n",
      "  [1 0 3 0 1]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 1 0 0]\n",
      "  [0 2 0 0 1]]\n",
      "\n",
      " [[0 0 1 2 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 3 0 0 0]\n",
      "  [0 1 0 0 0]\n",
      "  [1 1 0 3 1]\n",
      "  [0 0 0 0 0]\n",
      "  [0 0 0 1 2]\n",
      "  [2 0 0 0 2]\n",
      "  [0 2 0 2 0]\n",
      "  [2 1 0 1 0]\n",
      "  [2 0 2 0 1]\n",
      "  [1 0 0 1 0]\n",
      "  [1 1 0 0 0]\n",
      "  [0 0 0 0 0]\n",
      "  [3 0 0 0 0]\n",
      "  [1 1 0 1 0]\n",
      "  [0 2 0 0 0]\n",
      "  [0 1 0 1 1]\n",
      "  [0 1 1 0 0]\n",
      "  [1 1 1 0 0]\n",
      "  [0 2 0 0 3]\n",
      "  [0 0 1 0 0]\n",
      "  [0 0 0 1 0]\n",
      "  [0 0 0 1 0]\n",
      "  [3 1 0 1 0]\n",
      "  [0 1 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0]\n",
      "  [0 0 2 0 0]\n",
      "  [0 1 1 0 0]\n",
      "  [1 1 1 1 1]\n",
      "  [1 0 0 0 0]\n",
      "  [0 1 1 0 0]\n",
      "  [1 0 0 0 3]\n",
      "  [1 0 0 0 1]\n",
      "  [0 0 0 0 1]\n",
      "  [1 0 1 2 0]\n",
      "  [2 1 1 2 0]\n",
      "  [1 0 0 0 1]\n",
      "  [0 0 1 0 1]\n",
      "  [1 0 0 1 0]\n",
      "  [0 0 0 0 1]\n",
      "  [0 1 0 0 0]\n",
      "  [2 2 1 0 0]\n",
      "  [0 2 0 2 0]\n",
      "  [0 0 2 2 0]\n",
      "  [0 0 1 0 0]\n",
      "  [0 0 0 1 1]\n",
      "  [1 1 2 0 0]\n",
      "  [0 0 3 1 1]\n",
      "  [2 1 1 2 0]\n",
      "  [0 3 0 0 0]\n",
      "  [0 0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# 示例参数\n",
    "T = 7  # 时间步数量\n",
    "x_max_task_num = 3  # 最大值\n",
    "I_citys = 26  # 城市数量\n",
    "L_levels = 5  # 等级数量\n",
    "\n",
    "np.random.seed(42)\n",
    "lambda_il = np.random.rand(I_citys, L_levels)  # 生成率参数矩阵\n",
    "\n",
    "# 生成 arriving_tasks_i\n",
    "arriving_tasks_for_T = func7(T, x_max_task_num, lambda_il)\n",
    "print(arriving_tasks_for_T.shape, arriving_tasks_for_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "for j in range(J):\n",
    "    s0 = func1(I_citys, L_levels, W_workdays, M_servers, x_max_task_num, H_home_of_server, lambd)\n",
    "    xi = func7(T, x_max_task_num, lambda_il)\n",
    "    R_total_reward=[0]*T\n",
    "    for t in range(T):\n",
    "        action = func3(s0, L_server, H_home_of_server, r1_reward, c1_city_cost, c2)\n",
    "        R_total_reward[t] = reward(s0,action)\n",
    "        s1 = state_trans(s0,action,xi[t])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def reward(st,at):\n",
    "    \n",
    "\n",
    "\n",
    "def state_trans(S0,act,xi):   #状态转移\n",
    "    dic1 = {}\n",
    "    for i, row in enumerate(S0):\n",
    "        for j, reward in enumerate(row):\n",
    "            dic1[(i+1, j+1)] = reward\n",
    "    dic2 = {(x[1], x[2]): 1 for x in act if x[2] != 0}\n",
    "    S_A_cell = {}\n",
    "    for key in dic1:\n",
    "        if key in dic2:\n",
    "            S_A_cell[key] = dic1[key] - dic2[key]\n",
    "        else:\n",
    "            S_A_cell[key] = dic1[key]\n",
    "    S_A = [[0] *len(S0[0]) for _ in range(len(S0))]\n",
    "    for key, reward in S_A_cell.items():\n",
    "        S_A[key[0]-1][key[1]-1] = reward\n",
    "        \n",
    "    result = np.add(S_A,xi)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def state_trans(S0,act,xi):\n",
    "    dic1 = {}\n",
    "    for i, row in enumerate(S0):\n",
    "        for j, reward in enumerate(row):\n",
    "            dic1[(i+1, j+1)] = reward\n",
    "    dic2 = {(x[1], x[2]): 1 for x in act if x[2] != 0}\n",
    "    S_A_cell = {}\n",
    "    for key in dic1:\n",
    "        if key in dic2:\n",
    "            S_A_cell[key] = dic1[key] - dic2[key]\n",
    "        else:\n",
    "            S_A_cell[key] = dic1[key]\n",
    "    S_A = [[0] *len(S0[0]) for _ in range(len(S0))]\n",
    "    for key, reward in S_A_cell.items():\n",
    "        S_A[key[0]-1][key[1]-1] = reward\n",
    "        \n",
    "    result = np.add(S_A,xi)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0  1  0]\n",
      " [ 0  2  0  3  0]\n",
      " [ 1  2  1  3  3]\n",
      " [ 0  0  3  0  2]\n",
      " [ 0  3  0  2  3]\n",
      " [ 0 -1  1  0  1]\n",
      " [-1  1  0  1  2]\n",
      " [ 1  1  2  2  0]\n",
      " [ 1  0  2  0  0]\n",
      " [-1  0  1  1  3]\n",
      " [ 2  0  0  2  1]\n",
      " [ 0  1  0  0  1]\n",
      " [ 0 -1  1  0  0]\n",
      " [ 0  1  1  3  0]\n",
      " [ 0  4  0  0  0]\n",
      " [ 1  0  0  0  0]\n",
      " [ 1  0  1  1  4]\n",
      " [ 3  0  0  3  2]\n",
      " [ 0  0  1  1  1]\n",
      " [-1  0  1  0  0]\n",
      " [ 1  0  0  3  0]\n",
      " [ 0  0 -1  2  0]\n",
      " [ 0  1  2  1  0]\n",
      " [ 3  0  0  0 -1]\n",
      " [ 0  1  2  1  1]\n",
      " [ 0  0  0  2  3]]\n"
     ]
    }
   ],
   "source": [
    "S=S_t[0]\n",
    "act = A\n",
    "xi = arriving_tasks_for_T[0]\n",
    "tra = state_trans(S,act,xi)\n",
    "print(tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
